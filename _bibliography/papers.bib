---
---

@article{schroeder2024thinking,
  title={Thinking Deeper with Recursive Spawning (ThReaD)},
  author={Philip Schroeder, Nathaniel Morgan, Hongyin Luo, James Glass},
  abstract={Large language models (LLMs) have demonstrated impressive capabilities across diverse settings but still struggle as the length and complexity of the context increases. To address this challenge, we propose Thinking Recursively and Dynamically (ThReaD), a framework that adapts model generation by dynamically spawning new threads based on context. This method decomposes tasks into simpler sub-problems, enabling recursive problem-solving for complex tasks. ThReaD outperforms existing methods on benchmarks including ALFWorld, TextCraft, WebShop, DataCommons QA, and MIMIC-III ICU QA.},
  journal={NAACL},
  year={2025},
  url={https://doi.org/10.48550/arXiv.2405.17402},
  note={arXiv:2405.17402 [cs.CL]},
  selected={true}
}
@article{luo2025beyond,
  title={Beyond Context Limits: Subconscious Threads for Long-Horizon Reasoning},
  author={Hongyin Luo and Nathaniel Morgan and Tina Li and Derek Zhao and Ai Vy Ngo and Philip Schroeder and Lijie Yang and Assaf Ben-Kish and Jack O'Brien and James Glass},
  journal={arXiv},
  year={2025},
  abstract={To break the context limits of large language models (LLMs) that bottleneck reasoning accuracy and efficiency, we propose the Thread Inference Model (TIM), a family of LLMs trained for recursive and decompositional problem solving, and TIMRUN, an inference runtime enabling long-horizon structured reasoning beyond context limits. Together, TIM hosted on TIMRUN supports virtually unlimited working memory and multi-hop tool calls within a single language model inference, overcoming output limits, positional-embedding constraints, and GPU-memory bottlenecks. Performance is achieved by modeling natural language as reasoning trees measured by both length and depth instead of linear sequences. The reasoning trees consist of tasks with thoughts, recursive subtasks, and conclusions based on the concept we proposed in Schroeder et al, 2025. During generation, we maintain a working memory that retains only the key-value states of the most relevant context tokens, selected by a rule-based subtask-pruning mechanism, enabling reuse of positional embeddings and GPU memory pages throughout reasoning. Experimental results show that our system sustains high inference throughput, even when manipulating up to 90% of the KV cache in GPU memory. It also delivers accurate reasoning on mathematical tasks and handles information retrieval challenges that require long-horizon reasoning and multi-hop tool use.},
  url={https://doi.org/10.48550/arXiv.2507.16784},
  note={arXiv:2507.16784 [cs.CL], Research preview}
}
